# Fast Website Scanner

print("""'\033[1;92m'                                                 

╔╗──╔╦╗─╔╦╗──╔═╗─╔╦═══╦═══╦═══╦═╗─╔╗
║╚╗╔╝║║─║║║──║║╚╗║║╔═╗║╔═╗║╔═╗║║╚╗║║   
╚╗║║╔╣║─║║║──║╔╗╚╝║╚══╣║─╚╣║─║║╔╗╚╝║\033[4;37m
─║╚╝║║║─║║║─╔╣║╚╗║╠══╗║║─╔╣╚═╝║║╚╗║║
─╚╗╔╝║╚═╝║╚═╝║║─║║║╚═╝║╚═╝║╔═╗║║─║║║
──╚╝─╚═══╩═══╩╝─╚═╩═══╩═══╩╝─╚╩╝─╚═╝ 

               \033[1;32m Coded by: Mr.Dark Hcktvst""") 
print("\033[1;31;36m__________________________________________________________")
import requests
from termcolor import colored
from concurrent.futures import ThreadPoolExecutor

def check_website(website):
    try:
        response = requests.get(website)
        if response.status_code == 200:
            print(colored(f"{website} is reachable", 'green'))
        else:
            print(colored(f"{website} returned status code {response.status_code}", 'yellow'))
    except requests.exceptions.RequestException:
        print(colored(f"{website} is not reachable", 'red'))

def scan_websites(file_path):
    with open(file_path, 'r') as file:
        websites = [website.strip() for website in file.readlines()]

    with ThreadPoolExecutor(max_workers=10) as executor:
        executor.map(check_website, websites)

if __name__ == "__main__":
    file_path = input("Enter the file path containing websites: ")
    scan_websites(file_path)
